---
title: "Project3"
author: "Yuzhi Li and Annie Brinza"
date: "2022-11-10"
output: github_document
params:
  channel: "data_channel_is_lifestyle"
---

```{r GlobalOptions, include = FALSE}
options(knitr.duplicate.label = 'allow')
```

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(caret)
library(leaps)
library(rmarkdown)
```

# **Introduction**

In this project, we will analyze the data set about article shares based on various predictors. This data set includes fifty-eight predictors (both continuous and categorical variables) and one continuous response variable (shares). We will provide summary statistics and graphs for selected predictors in the summarizations part. Those are predictors that we think might have more effect on the number of shares, such as length of article tile, numbers of videos/images, the day that article published, etc. In the following section, we will work on building different models and model selection. The models we would like to fit are linear regression model, random forest model and boosted tree model. Cross-validation will be used for both random forest and boosted tree models. 

# **Data Import**

The first step we need to take is reading in the data. Then we will need to 
subset to our channel of interest. In this case, that channel is `r params$channel`.

```{r read_data}
df <- read_csv("data/OnlineNewsPopularity.csv", show_col_types = FALSE)
channel_df <- df %>% 
  filter(!!as.symbol(params$channel) == 1) %>% 
  select(-starts_with("data_channel_is"))
```


# **Summarizations**

In this section, we will produce some basic yet meaningful summary statistics and
plots about the training data we're working with. We will focus most closely on how
our data relates to the shares variable.

First, we will investigate the distribution of shares. This is important to understand
so all additional investigations can be contextualized. There will likely be
some outliers representing articles that went viral, resulting in a graph with a 
long right tail.

```{r share_distribution, message = FALSE, error = FALSE, warning = FALSE}
g <- ggplot(channel_df, aes(x = shares))
g + geom_histogram(fill = "mediumseagreen") + ggtitle("Distribution of Shares")
```

Now that we know the general shape of the distribution of shares, we will look 
into title length. The length of a title can affect whether someone
is willing to share an article, since if the title gets cut off when sharing, a 
person might be less likely to share. This could differ by channel. In the summary
below, pay attention to the average title length and see how it differs from the
other channels. 

```{r summary_1}
summary(channel_df$n_tokens_title)
```

To continue this investigation, we will plot title length against shares. If there
is a negative downward trend, that means the longer the title length, the less likely
a person is to share. If there is a positive upward trend, that means the longer
the title length, the more likely a person is to share. Check for outliers as well 
- see where they land on spectrum of title length. Those represent viral articles.

```{r graph_1}
g <- ggplot(channel_df, aes(x = n_tokens_title, y = shares))
g + geom_point() + xlab("Title Length") + ggtitle("Shares vs Title Length")
```
Next, we will investigate the length of the article itself. Compare the mean of
the article length across channels - do you see one where the articles are typically
longer?

```{r summary_2}
summary(channel_df$n_tokens_content)
```

We will plot the length of the content vs shares on a scatter plot. Once more, check
to see what the trend looks like. If it is a positive upward trend, then longer articles
will result in more shares. If it is a negative downward trend, then longer articles
will result in fewer shares. If it looks like a flat line, then article length doesn't
really affect shares. Once again, look for outliers. Those represent articles that
went viral - see where they tend to land when it comes to content length.

```{r graph_2}
g <- ggplot(channel_df, aes(x = n_tokens_content, y = shares))
g + geom_point() + xlab("Content Length") + ggtitle("Shares vs Content Length")
```
Now that we've investigated how length affects shares, we will dive into the actual
content of the article. First, we will look at how videos affect shares. In the summary,
compare the average number of videos to those of other channels to see which has the
most videos associated with the content. If the median is 0 but there is an average,
that means the majority of articles from this channel do not have any videos.

```{r summary_3}
summary(channel_df$num_videos)
```

Now we will see how the videos affect shares using a scatter plot. If there is a
positive upward trend, more videos in an article will result in more shares.
If there is a negative downward trend, more videos in an article will result in 
fewer shares.

```{r graph_3}
g <- ggplot(channel_df, aes(x = num_videos, y = shares))
g + geom_point() + xlab("Video Count") + ggtitle("Shares vs Video Count")
```

Sometimes, contents with more images are more attractive since visuals give us a better understanding. Now we will investigate how number of imagines affect the shares. First of all, we will take look at some data summaries. The mean and median of images will give us a general information about how likely the articles contain images. 

```{r summary_4}
summary(channel_df$num_imgs)
```

Now we would like to do a scatter plot of the number of shares verus the count of images. If the trend shows more images associate with high shares, then count of images may have positive effect on shares. Otherwise, if the trend of shares goes down as image increases, it may indicate a negative effect.

```{r graph_4}
g<- ggplot(channel_df, aes(x = num_imgs, y = shares))
  g + geom_point() + xlab("Image Count") + ggtitle("Shares vs Image Count")
```

Now I would like to see if rate of positive words can affect number of shares. First of all, let's looked at some summary statistics, where we can get an idea how likely the articles show positive words.

```{r summary_5}
summary(channel_df$rate_positive_words)
```

Now let's look into this more and make a plot between shares and rate of positive words. In order to see it more clear, I will cut the predictor in 0.1 length bins by 'cut_width' function. Then I will use the newly created variable as X axis and build a boxplot.By doing this way, we can exam the trend of shares for each interval of the rate.

```{r graph_5}
graph_5<- channel_df %>% mutate (bin=cut_width(rate_positive_words,width=0.1, boundary=0)) %>%
ggplot(aes(x = bin, y = shares)) +
  geom_boxplot (fill='#69b3a2') +
  xlab("rate_positive_words")
graph_5
```

Lastly, I also would like to see if weekday can affect the number of shares. Most people believe Monday is the busiest day during the week, when there will be more to catch up for work and school than other days. Is it true that people share less articles during Monday? Now let's summarize the shares based on predictor weekday so that we can visualize that. I

```{r summary_6}
a<-channel_df %>% filter(weekday_is_monday==1) %>% summarise(sum_shares=sum(shares))
b<-channel_df %>% filter(weekday_is_tuesday==1) %>% summarise(sum_shares=sum(shares))
c<-channel_df %>% filter(weekday_is_wednesday==1) %>% summarise(sum_shares=sum(shares))
d<-channel_df %>% filter(weekday_is_thursday==1) %>% summarise(sum_shares=sum(shares))
e<-channel_df %>% filter(weekday_is_friday==1) %>% summarise(sum_shares=sum(shares))
f<-channel_df %>% filter(weekday_is_saturday==1) %>% summarise(sum_shares=sum(shares))
g<-channel_df %>% filter(weekday_is_sunday==1) %>% summarise(sum_shares=sum(shares))
sum_shares_weekday<-data.frame(a,b,c,d,e,f,g)
names(sum_shares_weekday)<-c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
sum_shares_weekday
```

# **Modeling**

In order to do some modeling, first we will need to split the data into a training
and test set. First, we will remove URLs since they are just essentially IDs for
the articles.
```{r split_data}
# Setting seed for reproducibility
set.seed(24)
channel_df <- channel_df %>% select(-url, -timedelta)
train_index <- createDataPartition(channel_df$shares, p = 0.70, list = FALSE, times = 1)
train <- channel_df[train_index, ]
test <- channel_df[-train_index, ]
```

## Linear Regression Models

In this section, we will fit two linear regression models. The idea behind linear
regression is to essentially estimate an output using available features, fitting
the observations in a linear manner. The model is fit by minimizing the sum of squared
residuals, where a residual is the difference between the observed value and the 
predicted value. Linear regression can include interaction terms that represent
the interaction between two variables, as well as higher order terms such as
a variable squared.

### First Linear Regression Model

The first linear regression model we will train is a lasso linear regression model.
We have many predictors in this case, so lasso is helpful since it performs variable
selection.

```{r linear_model_1}
# Need to remove any highly correlated features for LR to work best
correlations <- cor(channel_df)
high_corr <-  findCorrelation(correlations, cutoff=0.5)  
high_corr <-  sort(high_corr)
reduced_data <-  channel_df[,-c(high_corr)]
train_reduced <- reduced_data[train_index, ]

# Fitting a lasso LR model since there are so many predictors
fit_lasso <- train(shares ~., data = train_reduced, method = "lasso",
             preProcess = c("center", "scale"), trControl = trainControl(method = "cv",
                                                                         number = 10))

```

### Second Linear Regression Model

For the second linear regression model, I will use Principal Component Analysis (PCA) to select the predictors before fitting linear regression model. In the process of fitting the first linear regression, we found that the data set has significant multicollinearity problem. PCA is commonly used to reduced the dimension of data by decomposing data into a number of independent factors. In the following section, I will combine PCA process and linear regression fit to one step.

```{r linear_model_2}
fit_lm_pca<- train(shares ~., data=train, method = "lm", preProcess = c("center", "scale", "pca"))
```

## Random Forest Model

In this section, we will fit a random forest model. Random forest models are an ensemble
model, using regression trees and a subset of predictors for each tree. They are an
extension of bagging models, where the algorithm resamples from the data, trains
a tree on this resample, and then calculates the prediction. This is done B times,
where B is generally a large number. The final predictions are then an average from
all of the trees. Random forest models do the same thing, except each tree only
uses a subset of predictors in order to reduce correlation amongst trees and thus
reduce variance.

```{r random_forest}
fit_rf <- train(shares ~., data = train, method = "rf", trControl = 
                   trainControl(method = "cv", number = 10), preProcess = 
                   c("center", "scale"))
```

## Boosted Tree Model

In this section, we will fit a boosted tree model. Boosted tree model is a combination of decision tree algorithms and boosting methods. Like Random Forest model we fit above, Boosted tree also repeatedly fit many decision trees to improve the accuracy of the model. However, Boosted tree uses boosting method in which the data selected is weighted in subsequent trees. In the procedure for regression tree, we will initialize predictions as 0 and find the residuals. Then fit a tree with d splits treating the residuals as the response. Following the same step, we update the predictions and residuals (for new prediction) for many times. There are three parameter we need to set in order to use this method: shrinkage parameter, number of splits and number of trees.

```{r boosted_tree}
fit_bt<- train(shares ~., data = train, method = "gbm",
               trControl = trainControl(method = "cv", number = 10),
               verbose = FALSE)
```

# **Comparison**

In this section, we will use the test data set to fit all candidate models we got from the above section. We will calculate the test Root Mean Squared Error (RMSE), $R^2$ and Mean Absolute Error (MAE) for each model. A model with good prediction will have small RMSE and MAE. $R^2$ also indicate how well the regression model fit. Ideally we would like to see $R^2$ value close to 1 for the two regression models. In this case, we will choose the model with smallest RMSE be the best fit model.

## Exam model fit on First Linear Regression Model

```{r fit_first_regression}
pred1<- predict(fit_lasso, newdata=test)
fit_result1<- postResample(pred1, obs = test$shares)
fit_result1
```

## Exam model fit on Second Linear Regression Model

```{r fit_second_regression}
pred2<- predict(fit_lm_pca, newdata=test)
fit_result2<- postResample(pred2, obs = test$shares)
fit_result2
```

## Exam model fit on Random Tree Model

```{r fit_random_forest}
pred3<- predict(fit_rf, newdata=test)
fit_result3<- postResample(pred3, obs = test$shares)
fit_result3
```

## Exam model fit on Boosted Tree Model

```{r fit_boosted_tree}
pred4<- predict(fit_bt, newdata=test)
fit_result4<- postResample(pred4, obs = test$shares)
fit_result4
```

## Choose the best fit model

I will arrange the model by RMSE value. The result will show the model name with the smallest RMSE.

```{r best_fit_model}
Model<- c("first_linear_regression", "second_linear_regression", "random_forest", "boosted_tree")
RMSE<- c(fit_result1[1], fit_result2[1], fit_result3[1], fit_result4[1])
result<- data.frame(Model, RMSE)
Best_model<- (result %>% arrange(desc(RMSE)))[4,1]
Best_model
```




