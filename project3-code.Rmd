---
title: "Project3"
author: "Yuzhi Li and Annie Brinza"
date: "2022-11-10"
output: html_document
params:
  channel: "data_channel_is_lifestyle"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(caret)
library(rmarkdown)
```

# **Introduction**

In this project, we will analyze the data set about article shares based on various predictors. This data set includes fifty-eight predictors (both continuous and categorical variables) and one continuous response variable (shares). We will provide summary statistics and graphs for selected predictors in the summarizations part. Those are predictors that we think might have more effect on the number of shares, such as length of article tile, numbers of videos/images, the day that article publisehd, etc. In the following section, we will work on building different models and model selection. The models we would like to fit are linear regression model, random forest model and boosted tree model. Cross-validation will be used for both random forest and boosted tree models. 

# **Data**

The first step we need to take is reading in the data. Then we will need to 
subset to our channel of interest. In this case, that channel is `params$channel`.

```{r read_data}
df <- read_csv("data/OnlineNewsPopularity.csv", show_col_types = FALSE)
channel_df <- df %>% 
  filter(data_channel_is_lifestyle == 1) %>% 
  select(-starts_with("data_channel_is"))
```


# **Summarizations**

In this section, we will produce some basic yet meaningful summary statistics and
plots about the training data we're working with. We will focus most closely on how
our data relates to the shares variable.

First, we will investigate the distribution of shares. This is important to understand
so all additional investigations can be contextualized. There will likely be
some outliers representing articles that went viral, resulting in a graph with a 
long right tail.

```{r share_distribution, message = FALSE, error = FALSE, warning = FALSE}
g <- ggplot(channel_df, aes(x = shares))
g + geom_histogram(fill = "mediumseagreen") + ggtitle("Distribution of Shares")
```

Now that we know the general shape of the distribution of shares, we will look 
into title length. The length of a title can affect whether someone
is willing to share an article, since if the title gets cut off when sharing, a 
person might be less likely to share. This could differ by channel. In the summary
below, pay attention to the average title length and see how it differs from the
other channels. 

```{r summary_1}
summary(channel_df$n_tokens_title)
```

To continue this investigation, we will plot title length against shares. If there
is a negative downward trend, that means the longer the title length, the less likely
a person is to share. If there is a positive upward trend, that means the longer
the title length, the more likely a person is to share. Check for outliers as well 
- see where they land on spectrum of title length. Those represent viral articles.

```{r graph_1}
g <- ggplot(channel_df, aes(x = n_tokens_title, y = shares))
g + geom_point() + xlab("Title Length") + ggtitle("Shares vs Title Length")
```
Next, we will investigate the length of the article itself. Compare the mean of
the article length across channels - do you see one where the articles are typically
longer?

```{r summary_2}
summary(channel_df$n_tokens_content)
```

We will plot the length of the content vs shares on a scatter plot. Once more, check
to see what the trend looks like. If it is a positive upward trend, then longer articles
will result in more shares. If it is a negative downward trend, then longer articles
will result in fewer shares. If it looks like a flat line, then article length doesn't
really affect shares. Once again, look for outliers. Those represent articles that
went viral - see where they tend to land when it comes to content length.

```{r graph_2}
g <- ggplot(channel_df, aes(x = n_tokens_content, y = shares))
g + geom_point() + xlab("Content Length") + ggtitle("Shares vs Content Length")
```
Now that we've investigated how length affects shares, we will dive into the actual
content of the article. First, we will look at how videos affect shares. In the summary,
compare the average number of videos to those of other channels to see which has the
most videos associated with the content. If the median is 0 but there is an average,
that means the majority of articles from this channel do not have any videos.

```{r summary_3}
summary(channel_df$num_videos)
```

Now we will see how the videos affect shares using a scatter plot. If there is a
positive upward trend, more videos in an article will result in more shares.
If there is a negative downward trend, more videos in an article will result in 
fewer shares.

```{r graph_3}
g <- ggplot(channel_df, aes(x = num_videos, y = shares))
g + geom_point() + xlab("Video Count") + ggtitle("Shares vs Video Count")
```

Sometimes, contents with more images are more attractive since visuals give us a better understanding. Now we will investigate how number of imagines affect the shares. First of all, we will take look at some data summaries.

```{r summary_4}
summary(channel_df$num_imgs)
```

It looks like most articles do not have many images since the median value is only 1. The mean value is close to 5, there might be a few articles with a large amount of pictures.

Now we would like to plot the number of shares as the image count changes by a scatter plot.

```{r graph_4}
g<- ggplot(channel_df, aes(x = num_imgs, y = shares))
  g + geom_point() + xlab("Image Count") + ggtitle("Shares vs Image Count")
```

As you can tell, the amount of shares are not necessarily relateing to the number of images. There are even some high shares when image count close to 0.

Now I would like to see if rate of positive words can affect number of shares. First of all, let's looked at some summary statistics.

```{r summary_5}
summary(channel_df$rate_positive_words)
```

It looks like most articles have rate of positive words more than 50% for this channel. Let's look into this more and make a plot between shares and rate of positive words. In order to see it more clear, I will cut the predictor in 0.1 length bins by 'cut_width' function. Then I will use the newly created variable as X axis and build a boxplot.

```{r graph_5}
graph_5<- channel_df %>% mutate (bin=cut_width(rate_positive_words,width=0.1, boundary=0)) %>%
ggplot(aes(x = bin, y = shares)) +
  geom_boxplot (fill='#69b3a2') +
  xlab("rate_positive_words")
graph_5
```
  
Based on the boxplot above, it looks like there are more shares when rate of positive words around 0.6 to 0.8.

Lastly, I also would like to see if weekday can affect the number of shares. Most people believe Monday is the busiest day during the week, when there might be more to catch up than other days of the week. Is it true that people share less articles during Monday? Now let's summarize the shares based on weekday (the day that article is published during the week) so that we can visualize that.

```{r summary_6}
a<-channel_df %>% filter(weekday_is_monday==1) %>% summarise(sum_shares=sum(shares))
b<-channel_df %>% filter(weekday_is_tuesday==1) %>% summarise(sum_shares=sum(shares))
c<-channel_df %>% filter(weekday_is_wednesday==1) %>% summarise(sum_shares=sum(shares))
d<-channel_df %>% filter(weekday_is_thursday==1) %>% summarise(sum_shares=sum(shares))
e<-channel_df %>% filter(weekday_is_friday==1) %>% summarise(sum_shares=sum(shares))
f<-channel_df %>% filter(weekday_is_saturday==1) %>% summarise(sum_shares=sum(shares))
g<-channel_df %>% filter(weekday_is_sunday==1) %>% summarise(sum_shares=sum(shares))
sum_shares_weekday<-data.frame(a,b,c,d,e,f,g)
names(sum_shares_weekday)<-c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun")
sum_shares_weekday
```

It's surprisingly that Saturday is the date that has the lowest shares during the week. A reason might be people will have more activities to do during weekend.

# **Modeling**
In order to do some modeling, first we will need to split the data into a training
and test set. First, we will remove URLs since they are just essentially IDs for
the articles.
```{r split_data}
# Setting seed for reproducibility
set.seed(24)
channel_df <- channel_df %>% select(-url)
train_index <- createDataPartition(channel_df$shares, p = 0.70, list = FALSE, times = 1)
train <- channel_df[train_index, ]
test <- channel_df[-train_index, ]
```

## Linear Regression Models
In this section, we will fit two linear regression models. The idea behind linear
regression is to essentially estimate an output using available features, fitting
the observations in a linear manner. The model is fit by minimizing the sum of squared
residuals, where a residual is the difference between the observed value and the 
predicted value. Linear regression can include interaction terms that represent
the interaction between two variables, as well as higher order terms such as
a variable squared.

The first linear regression model we will train is a lasso linear regression model.
We have many predictors in this case, so lasso is helpful since it performs variable
selection.

```{r linear_model_1}
# Need to remove any highly correlated features for LR to work best
correlations <- cor(channel_df)
high_corr <-  findCorrelation(correlations, cutoff=0.5)  
high_corr <-  sort(high_corr)
reduced_data <-  channel_df[,-c(high_corr)]
train_reduced <- reduced_data[train_index, ]

# Fitting a lasso LR model since there are so many predictors
fit_lasso <- train(shares ~., data = train_reduced, method = "lasso",
             preProcess = c("center", "scale"), trControl = trainControl(method = "cv",
                                                                         number = 10))

```



## Random Forest Model
In this section, we will fit a random forest model. Random forest models are an ensemble
model, using regression trees and a subset of predictors for each tree. They are an
extension of bagging models, where the algorithm resamples from the data, trains
a tree on this resample, and then calculates the prediction. This is done B times,
where B is generally a large number. The final predictions are then an average from
all of the trees. Random forest models do the same thing, except each tree only
uses a subset of predictors in order to reduce correlation amongst trees and thus
reduce variance.

```{r random_forest}
fit_rf <- train(shares ~., data = train, method = "rf", trControl = 
                   trainControl(method = "cv", number = 10), preProcess = 
                   c("center", "scale"))
```


# **Comparison**